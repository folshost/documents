%

\label{chap:intro}
\paragraph{}
Machine learning libraries like Google's Tensorflow, or Theano provide a mathematical backbone for the linear algebra operations which are at the heart of almost all statistical and machine learning methods. Without them it would be much more difficult to flexibly build and deploy complex machine learning models. As machine learning models continue to become more and more complex, and require larger amounts of data to train, the continued absence of libraries with robust integration of distributed and high performance systems unnecessarily hamstrings forward advancement. Phylanx \cite{Phylanx}, the distributed array processing toolkit, was created to address these problems. Similar to how other libraries like Tensorflow seek to expose a Python api to enable easier uptake by domain experts, Phylanx was built to enable Python code to be disassembled and optimized for running in a distributed environment, requiring no more complicated knowledge of Python than Numpy or Keras. Under the hood, Phylanx makes use of HPX\cite{Heller2017}, the C++ distributed runtime developed by the Ste||ar Group, to enable distributed functionality. For optimized shared memory linear algebra performance, Phylanx utilizes the Blaze library\cite{blaze}, developed by Klaus Iglberger. Prior to the work that this report references Phylanx had capitalized on only a few opportunities for distributed functionality, primarily matrix vector products, and matrix addition. In our work on Phylanx we extended this to include two matrix multiplication algorithms, a generalized version allowing irregular matrix tile sizes, and a modification on Cannon's algorithm.
\paragraph{}
This work was also undertaken with a goal of learning about tiling optimization, and the opportunities for it in a linear algebra system. When we make reference to 'tiling' we mean the result of distributing data across multiple computational nodes so that each data structure which has been distributed (most often a vector or matrix) is composed of many different 'tiles', or subsets, which are placed on many different machines in a cluster.
\paragraph{}
 From preliminary testing we found that our distributed primitives (functions) ran faster than the serial versions in a shared memory distributed environment\footnote{That is, running multiple processes at the same time on one machine, requiring each process to go through HPX's TCP/IP layer to communicate with one another. This meant that the computation was distributed in a sense, but also requires further distributed testing in a cluster environment to confirm our results}. In Chapter 5 we discuss these results in detail.

%
